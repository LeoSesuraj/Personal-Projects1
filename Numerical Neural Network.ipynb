{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "# Normalize the input data to the range [0, 1], since there are 255 pixel values\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Flatten the input data (from 28x28 images to 784 feature vectors) and transpose\n",
    "X_train = X_train.reshape(X_train.shape[0], -1).T  # Shape: (784, num_examples)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTIONAL -Display Number\n",
    "\n",
    "def display_live_number(X, index):\n",
    "    # plt.clf()  # Clear the current plot\n",
    "    image = X[:, index].reshape(28, 28)  # Reshape the flattened image back to 28x28\n",
    "    \n",
    "    # Display the image\n",
    "    plt.imshow(image, cmap=\"gray\")\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert labels to one-hot encoding\n",
    "\n",
    "def one_hot_encode(Y, num_classes):\n",
    "    Y = Y.reshape(1, Y.shape[0])  # Reshape to (1, num_examples)\n",
    "    Y_one_hot = np.zeros((num_classes, Y.shape[1]))  # Initialize a zero matrix with shape (num_classes, num_examples)\n",
    "    Y_one_hot[Y.astype(int), np.arange(Y.shape[1])] = 1  # Set the appropriate indices to 1 for each class label\n",
    "    return Y_one_hot\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that initilizes the weights, will only need for training\n",
    "\n",
    "def initialize_parameters(input_size, hidden_size, output_size):\n",
    "    W1 = np.random.randn(hidden_size, input_size) * np.sqrt(2./input_size)  # shape: (hidden_size, input_size)\n",
    "    b1 = np.zeros((hidden_size, 1))  # shape: (hidden_size, 1)\n",
    "    W2 = np.random.randn(hidden_size, hidden_size) * np.sqrt(2./hidden_size)  # shape: (hidden_size, hidden_size)\n",
    "    b2 = np.zeros((hidden_size, 1))  # shape: (hidden_size, 1)\n",
    "    W3 = np.random.randn(output_size, hidden_size) * np.sqrt(2./hidden_size)  # shape: (output_size, hidden_size)\n",
    "    b3 = np.zeros((output_size, 1))  # shape: (output_size, 1)\n",
    "\n",
    "    return W1, b1, W2, b2, W3, b3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relu call to change data from linear representation to a more complex version so the deep learning process is more intricate\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0, Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for the last layer and for the output, the softmax will convert Z into probabilities based on the formula\n",
    "\n",
    "def softmax(Z):\n",
    "    # to highlight the more confident probabilities and to make positive probabilities:\n",
    "    exp_Z = np.exp(Z - np.max(Z, axis=0, keepdims=True))  # For numerical stability\n",
    "    return exp_Z / np.sum(exp_Z, axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward pass algorithm\n",
    "\n",
    "def forward_pass(X, W1, b1, W2, b2, W3, b3):\n",
    "    # First hidden layer (Input -> Hidden Layer 1)\n",
    "    Z1 = np.dot(W1, X) + b1  # Z1 shape: (hidden_size, num_examples)\n",
    "    A1 = relu(Z1)\n",
    "\n",
    "    # Second hidden layer (Hidden Layer 1 -> Hidden Layer 2)\n",
    "    Z2 = np.dot(W2, A1) + b2  # Z2 shape: (hidden_size, num_examples)\n",
    "    A2 = relu(Z2)\n",
    "    \n",
    "    # Output layer (Hidden Layer 2 -> Output)\n",
    "    Z3 = np.dot(W3, A2) + b3  # Z3 shape: (output_size, num_examples)\n",
    "    A3 = softmax(Z3)\n",
    "\n",
    "    return Z1, A1, Z2, A2, Z3, A3  # Return the output probabilities and intermediate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss calculation that finds the loss and how well the predicted values align with the true values\n",
    "\n",
    "def compute_loss(A3, Y):\n",
    "    m = Y.shape[1]  # Number of examples\n",
    "    epsilon = 1e-15  # Small constant to avoid log(0)\n",
    "    \n",
    "    # Compute the log likelihood for the correct class (one-hot encoded labels)\n",
    "    log_probs = np.multiply(np.log(A3 + epsilon), Y)  # Element-wise multiplication\n",
    "    cost = -np.sum(log_probs) / m  # Average loss over the batch\n",
    "    \n",
    "    print(\"Accuracy: {}\".format(np.mean(np.argmax(A3,axis=0) == np.argmax(Y,axis=0))*100))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#derivative function that returns 0 if the Z is less than 0\n",
    "def relu_derivative(Z):\n",
    "    return Z > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backpropagation, going backwards in the neural network and calculating the slope and direction to move it to adjust based on the loss\n",
    "#Backpropagation only finds the direction (slope) of the movement, not by actually finding how much it needs to move, that's the learning rate.\n",
    "\n",
    "def backpropagation(X, Y, parameters, cache):\n",
    "    m = X.shape[1]  # Number of examples (batch size)\n",
    "\n",
    "    # cache is a type of memory retreival, where we are accessing the values of the following:\n",
    "    A1 = cache['A1']  # relu version of weighted matrix layer 1\n",
    "    A2 = cache['A2']  # relu version of weighted matrix layer 2\n",
    "    A3 = cache['A3']  # softmax version of weighted matrix layer 3 (output layer)\n",
    "    Z1 = cache['Z1']  # weighted matrix pre-relu layer 1\n",
    "    Z2 = cache['Z2']  # weighted matrix pre-relu layer 2\n",
    "\n",
    "    W1, W2, W3 = parameters['W1'], parameters['W2'], parameters['W3']\n",
    "\n",
    "    # Layer 3: Compute the gradient for layer 3, output layer\n",
    "    dZ3 = A3 - Y\n",
    "\n",
    "    # Layer 3: Compute the gradients for W3 and b3 (output layer)\n",
    "    dW3 = np.dot(dZ3, A2.T) / m  # Gradient of W3\n",
    "    db3 = np.sum(dZ3, axis=1, keepdims=True) / m  # Gradient of b3\n",
    "\n",
    "    # Layer 2: Backpropagate the error to the second hidden layer (Z2)\n",
    "    dZ2 = np.dot(W3.T, dZ3) * relu_derivative(Z2)  # Gradient of Z2\n",
    "\n",
    "    # Layer 2: Compute the gradients for W2 and b2 (second hidden layer)\n",
    "    dW2 = np.dot(dZ2, A1.T) / m  # Gradient of W2\n",
    "    db2 = np.sum(dZ2, axis=1, keepdims=True) / m  # Gradient of b2\n",
    "\n",
    "    # Layer 1: Backpropagate the error to the first hidden layer (Z1)\n",
    "    dZ1 = np.dot(W2.T, dZ2) * relu_derivative(Z1)  # Gradient of Z1\n",
    "    \n",
    "    # Layer 1: Compute the gradients for W1 and b1 (first hidden layer)\n",
    "    dW1 = np.dot(dZ1, X.T) / m  # Gradient of W1\n",
    "    db1 = np.sum(dZ1, axis=1, keepdims=True) / m  # Gradient of b1\n",
    "\n",
    "    gradients = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2, \"dW3\": dW3, \"db3\": db3}\n",
    "\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the actual gradient descent step where the weights and biases are updated based on a learning rate (usually beginning at 0.01)\n",
    "#The gradient descent step will repeat multiple times, each time adjusting the weight and bias closer and closer for precision\n",
    "\n",
    "def gradient_descent(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, learning_rate):\n",
    "    # Update weights and biases using gradient descent\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    W3 -= learning_rate * dW3\n",
    "    b3 -= learning_rate * db3\n",
    "    \n",
    "    return W1, b1, W2, b2, W3, b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the algorithm\n",
    "def train(X, Y, input_size, hidden_layer_size, output_size, num_iterations, learning_rate, batch_size=128):\n",
    "    # Initialize the parameters (weights and biases)\n",
    "    W1, b1, W2, b2, W3, b3 = initialize_parameters(input_size, hidden_layer_size, output_size)\n",
    "    m = X.shape[1]\n",
    "    costs = []\n",
    "    \n",
    "    # Train for the specified number of iterations\n",
    "    for iteration in range(num_iterations):\n",
    "        # Mini-batch implementation\n",
    "        for batch_start in range(0, m, batch_size):\n",
    "            batch_end = min(batch_start + batch_size, m)\n",
    "            X_batch = X[:, batch_start:batch_end]\n",
    "            Y_batch = Y[:, batch_start:batch_end]\n",
    "            \n",
    "            # Forward propagation\n",
    "            Z1, A1, Z2, A2, Z3, A3 = forward_pass(X_batch, W1, b1, W2, b2, W3, b3)\n",
    "            \n",
    "            # Compute loss for monitoring\n",
    "            if batch_start == 0 and (iteration+1) % 1 == 0:\n",
    "                cost = compute_loss(A3, Y_batch)\n",
    "                costs.append(cost)\n",
    "                print(f\"Iteration {iteration+1}, Cost: {cost}\")\n",
    "            \n",
    "            # Backpropagation to compute gradients\n",
    "            gradients = backpropagation(X_batch, Y_batch, \n",
    "                                      {\"W1\": W1, \"W2\": W2, \"W3\": W3},\n",
    "                                      {'A1': A1, 'A2': A2, 'A3': A3, 'Z1': Z1, 'Z2': Z2})\n",
    "            \n",
    "            # Update parameters using gradient descent\n",
    "            W1, b1, W2, b2, W3, b3 = gradient_descent(\n",
    "                W1, b1, W2, b2, W3, b3,\n",
    "                gradients['dW1'], gradients['db1'],\n",
    "                gradients['dW2'], gradients['db2'],\n",
    "                gradients['dW3'], gradients['db3'],\n",
    "                learning_rate\n",
    "            )\n",
    "    \n",
    "    return W1, b1, W2, b2, W3, b3, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 10.9375\n",
      "Iteration 1, Cost: 2.329885037184076\n",
      "Accuracy: 96.875\n",
      "Iteration 2, Cost: 0.1559679900338018\n",
      "Accuracy: 97.65625\n",
      "Iteration 3, Cost: 0.10401677301939699\n",
      "Accuracy: 97.65625\n",
      "Iteration 4, Cost: 0.08172242023886588\n",
      "Accuracy: 98.4375\n",
      "Iteration 5, Cost: 0.06844367084368308\n",
      "Accuracy: 98.4375\n",
      "Iteration 6, Cost: 0.05905333505115837\n",
      "Accuracy: 99.21875\n",
      "Iteration 7, Cost: 0.04905936391301063\n",
      "Accuracy: 99.21875\n",
      "Iteration 8, Cost: 0.04163234415167916\n",
      "Accuracy: 99.21875\n",
      "Iteration 9, Cost: 0.03642851970775748\n",
      "Accuracy: 99.21875\n",
      "Iteration 10, Cost: 0.03297071303401823\n"
     ]
    }
   ],
   "source": [
    "#Main Class to train and run\n",
    "\n",
    "input_size = 784  # each image is 28x28 pixels\n",
    "hidden_layer_size = 128  # number of neurons in each hidden layer (increased from 20 for better performance)\n",
    "output_size = 10  # 0-9 possible values\n",
    "num_iterations = 10  # running the training 10 times to modify the matrices accordingly\n",
    "learning_rate = 0.1\n",
    "batch_size = 128\n",
    "\n",
    "# Prepare data with one-hot encoding\n",
    "Y_train = one_hot_encode(Y_train, output_size)\n",
    "Y_test = one_hot_encode(Y_test, output_size)\n",
    "\n",
    "# Train the model\n",
    "W1, b1, W2, b2, W3, b3, costs = train(X_train, Y_train, input_size, hidden_layer_size, output_size, num_iterations, learning_rate, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.39\n",
      "\n",
      "Actual: 6 Predicted: 5\n",
      "\n",
      "Actual: 4 Predicted: 2\n",
      "\n",
      "Actual: 6 Predicted: 0\n",
      "\n",
      "Actual: 8 Predicted: 4\n",
      "\n",
      "Actual: 2 Predicted: 7\n",
      "\n",
      "Actual: 6 Predicted: 0\n",
      "\n",
      "Actual: 3 Predicted: 5\n",
      "\n",
      "Actual: 8 Predicted: 0\n",
      "\n",
      "Actual: 8 Predicted: 2\n",
      "\n",
      "Actual: 4 Predicted: 6\n",
      "\n",
      "Actual: 3 Predicted: 9\n",
      "\n",
      "Actual: 2 Predicted: 6\n",
      "\n",
      "Actual: 2 Predicted: 1\n",
      "\n",
      "Actual: 7 Predicted: 3\n",
      "\n",
      "Actual: 7 Predicted: 9\n",
      "\n",
      "Actual: 8 Predicted: 4\n",
      "\n",
      "Actual: 9 Predicted: 4\n",
      "\n",
      "Actual: 8 Predicted: 5\n",
      "\n",
      "Actual: 8 Predicted: 9\n",
      "\n",
      "Actual: 5 Predicted: 4\n",
      "\n",
      "Actual: 6 Predicted: 0\n",
      "\n",
      "Actual: 6 Predicted: 5\n",
      "\n",
      "Actual: 7 Predicted: 2\n",
      "\n",
      "Actual: 6 Predicted: 8\n",
      "\n",
      "Actual: 4 Predicted: 6\n",
      "\n",
      "Actual: 7 Predicted: 8\n",
      "\n",
      "Actual: 3 Predicted: 5\n",
      "\n",
      "Actual: 6 Predicted: 1\n",
      "\n",
      "Actual: 6 Predicted: 5\n",
      "\n",
      "Actual: 7 Predicted: 9\n",
      "\n",
      "Actual: 2 Predicted: 4\n",
      "\n",
      "Actual: 7 Predicted: 2\n",
      "\n",
      "Actual: 9 Predicted: 4\n",
      "\n",
      "Actual: 4 Predicted: 9\n",
      "\n",
      "Actual: 9 Predicted: 0\n",
      "\n",
      "Actual: 7 Predicted: 1\n",
      "\n",
      "Actual: 8 Predicted: 3\n",
      "\n",
      "Actual: 7 Predicted: 9\n",
      "\n",
      "Actual: 8 Predicted: 3\n",
      "\n",
      "Actual: 3 Predicted: 5\n",
      "\n",
      "Actual: 7 Predicted: 0\n",
      "\n",
      "Actual: 7 Predicted: 9\n",
      "\n",
      "Actual: 7 Predicted: 1\n",
      "\n",
      "Actual: 7 Predicted: 9\n",
      "\n",
      "Actual: 1 Predicted: 5\n",
      "\n",
      "Actual: 8 Predicted: 7\n",
      "\n",
      "Actual: 4 Predicted: 6\n",
      "\n",
      "Actual: 9 Predicted: 3\n",
      "\n",
      "Actual: 6 Predicted: 5\n",
      "\n",
      "Actual: 2 Predicted: 3\n",
      "\n",
      "Actual: 6 Predicted: 5\n",
      "\n",
      "Actual: 7 Predicted: 3\n",
      "\n",
      "Actual: 3 Predicted: 7\n",
      "\n",
      "Actual: 8 Predicted: 0\n",
      "\n",
      "Actual: 7 Predicted: 9\n",
      "\n",
      "Actual: 7 Predicted: 2\n",
      "\n",
      "Actual: 2 Predicted: 9\n",
      "\n",
      "Actual: 8 Predicted: 7\n",
      "\n",
      "Actual: 7 Predicted: 9\n",
      "\n",
      "Actual: 9 Predicted: 4\n",
      "\n",
      "Actual: 7 Predicted: 8\n",
      "\n",
      "Actual: 9 Predicted: 8\n",
      "\n",
      "Actual: 6 Predicted: 5\n",
      "\n",
      "Actual: 2 Predicted: 0\n",
      "\n",
      "Actual: 7 Predicted: 2\n",
      "\n",
      "Actual: 7 Predicted: 9\n",
      "\n",
      "Actual: 5 Predicted: 4\n",
      "\n",
      "Actual: 2 Predicted: 7\n",
      "\n",
      "Actual: 6 Predicted: 5\n",
      "\n",
      "Actual: 4 Predicted: 9\n",
      "\n",
      "Actual: 7 Predicted: 9\n",
      "\n",
      "Actual: 8 Predicted: 1\n",
      "\n",
      "Actual: 2 Predicted: 0\n",
      "\n",
      "Actual: 3 Predicted: 9\n",
      "\n",
      "Actual: 6 Predicted: 0\n",
      "\n",
      "Actual: 6 Predicted: 1\n",
      "\n",
      "Actual: 0 Predicted: 5\n",
      "\n",
      "Actual: 9 Predicted: 1\n",
      "\n",
      "Actual: 8 Predicted: 0\n",
      "\n",
      "Actual: 9 Predicted: 0\n",
      "\n",
      "Actual: 0 Predicted: 5\n",
      "\n",
      "Actual: 5 Predicted: 9\n",
      "\n",
      "Actual: 9 Predicted: 1\n",
      "\n",
      "Actual: 9 Predicted: 1\n",
      "\n",
      "Actual: 9 Predicted: 4\n",
      "\n",
      "Actual: 2 Predicted: 1\n",
      "\n",
      "Actual: 2 Predicted: 4\n",
      "\n",
      "Actual: 5 Predicted: 8\n",
      "\n",
      "Actual: 5 Predicted: 3\n",
      "\n",
      "Actual: 7 Predicted: 1\n",
      "\n",
      "Actual: 9 Predicted: 0\n",
      "\n",
      "Actual: 6 Predicted: 1\n",
      "\n",
      "Actual: 9 Predicted: 4\n",
      "\n",
      "Actual: 6 Predicted: 5\n",
      "\n",
      "Actual: 7 Predicted: 4\n",
      "\n",
      "Actual: 9 Predicted: 4\n",
      "\n",
      "Actual: 4 Predicted: 7\n",
      "\n",
      "Actual: 8 Predicted: 0\n",
      "\n",
      "Actual: 7 Predicted: 3\n",
      "\n",
      "Actual: 3 Predicted: 2\n",
      "\n",
      "Actual: 9 Predicted: 5\n",
      "\n",
      "Actual: 3 Predicted: 5\n",
      "\n",
      "Actual: 6 Predicted: 5\n",
      "\n",
      "Actual: 9 Predicted: 1\n",
      "\n",
      "Actual: 8 Predicted: 5\n",
      "\n",
      "Actual: 9 Predicted: 7\n",
      "\n",
      "Actual: 8 Predicted: 5\n",
      "\n",
      "Actual: 1 Predicted: 2\n",
      "\n",
      "Actual: 4 Predicted: 6\n",
      "\n",
      "Actual: 5 Predicted: 9\n",
      "\n",
      "Actual: 8 Predicted: 3\n",
      "\n",
      "Actual: 6 Predicted: 5\n",
      "\n",
      "Actual: 8 Predicted: 5\n",
      "\n",
      "Actual: 7 Predicted: 9\n",
      "\n",
      "Actual: 6 Predicted: 0\n",
      "\n",
      "Actual: 2 Predicted: 1\n",
      "\n",
      "Actual: 9 Predicted: 1\n",
      "\n",
      "Actual: 6 Predicted: 4\n",
      "\n",
      "Actual: 3 Predicted: 2\n",
      "\n",
      "Actual: 5 Predicted: 0\n",
      "\n",
      "Actual: 8 Predicted: 5\n",
      "\n",
      "Actual: 8 Predicted: 5\n",
      "\n",
      "Actual: 9 Predicted: 3\n",
      "\n",
      "Actual: 2 Predicted: 3\n",
      "\n",
      "Actual: 6 Predicted: 5\n",
      "\n",
      "Actual: 4 Predicted: 9\n",
      "\n",
      "Actual: 8 Predicted: 3\n",
      "\n",
      "Actual: 7 Predicted: 3\n",
      "\n",
      "Actual: 5 Predicted: 8\n",
      "\n",
      "Actual: 4 Predicted: 6\n",
      "\n",
      "Actual: 2 Predicted: 8\n",
      "\n",
      "Actual: 7 Predicted: 8\n",
      "\n",
      "Actual: 2 Predicted: 3\n",
      "\n",
      "Actual: 0 Predicted: 4\n",
      "\n",
      "Actual: 7 Predicted: 4\n",
      "\n",
      "Actual: 6 Predicted: 5\n",
      "\n",
      "Actual: 9 Predicted: 4\n",
      "\n",
      "Actual: 2 Predicted: 8\n",
      "\n",
      "Actual: 1 Predicted: 3\n",
      "\n",
      "Actual: 4 Predicted: 6\n",
      "\n",
      "Actual: 8 Predicted: 2\n",
      "\n",
      "Actual: 7 Predicted: 1\n",
      "\n",
      "Actual: 9 Predicted: 4\n",
      "\n",
      "Actual: 7 Predicted: 4\n",
      "\n",
      "Actual: 7 Predicted: 9\n",
      "\n",
      "Actual: 6 Predicted: 5\n",
      "\n",
      "Actual: 0 Predicted: 2\n",
      "\n",
      "Actual: 9 Predicted: 3\n",
      "\n",
      "Actual: 9 Predicted: 0\n",
      "\n",
      "Actual: 2 Predicted: 7\n",
      "\n",
      "Actual: 7 Predicted: 9\n",
      "\n",
      "Actual: 6 Predicted: 5\n",
      "\n",
      "Actual: 2 Predicted: 8\n",
      "\n",
      "Actual: 2 Predicted: 7\n",
      "\n",
      "Actual: 9 Predicted: 5\n",
      "\n",
      "Actual: 9 Predicted: 4\n",
      "\n",
      "Actual: 8 Predicted: 5\n",
      "\n",
      "Actual: 7 Predicted: 4\n",
      "\n",
      "Actual: 9 Predicted: 4\n",
      "\n",
      "Actual: 8 Predicted: 7\n",
      "\n",
      "Actual: 6 Predicted: 5\n",
      "\n",
      "Actual: 7 Predicted: 9\n",
      "\n",
      "Actual: 8 Predicted: 4\n",
      "\n",
      "Actual: 2 Predicted: 4\n",
      "\n",
      "Actual: 8 Predicted: 9\n",
      "\n",
      "Actual: 7 Predicted: 2\n",
      "\n",
      "Actual: 8 Predicted: 3\n",
      "\n",
      "Actual: 6 Predicted: 7\n",
      "\n",
      "Actual: 9 Predicted: 6\n",
      "\n",
      "Actual: 6 Predicted: 5\n",
      "\n",
      "Actual: 4 Predicted: 9\n",
      "\n",
      "Actual: 9 Predicted: 5\n",
      "\n",
      "Actual: 2 Predicted: 4\n",
      "\n",
      "Actual: 8 Predicted: 6\n",
      "\n",
      "Actual: 0 Predicted: 8\n",
      "\n",
      "Actual: 7 Predicted: 1\n",
      "\n",
      "Actual: 8 Predicted: 4\n",
      "\n",
      "Actual: 7 Predicted: 4\n",
      "\n",
      "Actual: 3 Predicted: 5\n",
      "\n",
      "Actual: 0 Predicted: 5\n",
      "\n",
      "Actual: 1 Predicted: 6\n",
      "\n",
      "Actual: 4 Predicted: 6\n",
      "\n",
      "Actual: 1 Predicted: 8\n",
      "\n",
      "Actual: 7 Predicted: 9\n",
      "\n",
      "Actual: 1 Predicted: 5\n",
      "\n",
      "Actual: 7 Predicted: 9\n",
      "\n",
      "Actual: 7 Predicted: 9\n",
      "\n",
      "Actual: 3 Predicted: 7\n",
      "\n",
      "Actual: 8 Predicted: 5\n",
      "\n",
      "Actual: 7 Predicted: 9\n",
      "\n",
      "Actual: 7 Predicted: 4\n",
      "\n",
      "Actual: 7 Predicted: 0\n",
      "\n",
      "Actual: 4 Predicted: 0\n",
      "\n",
      "Actual: 4 Predicted: 9\n",
      "\n",
      "Actual: 5 Predicted: 3\n",
      "\n",
      "Actual: 3 Predicted: 8\n",
      "\n",
      "Actual: 5 Predicted: 3\n",
      "\n",
      "Actual: 3 Predicted: 8\n",
      "\n",
      "Actual: 3 Predicted: 9\n",
      "\n",
      "Actual: 2 Predicted: 0\n",
      "\n",
      "Actual: 3 Predicted: 9\n",
      "\n",
      "Actual: 8 Predicted: 3\n",
      "\n",
      "Actual: 3 Predicted: 9\n",
      "\n",
      "Actual: 9 Predicted: 3\n",
      "\n",
      "Actual: 9 Predicted: 0\n",
      "\n",
      "Actual: 3 Predicted: 5\n",
      "\n",
      "Actual: 8 Predicted: 9\n",
      "\n",
      "Actual: 6 Predicted: 2\n",
      "\n",
      "Actual: 9 Predicted: 4\n",
      "\n",
      "Actual: 0 Predicted: 9\n",
      "\n",
      "Actual: 8 Predicted: 9\n",
      "\n",
      "Actual: 0 Predicted: 5\n",
      "\n",
      "Actual: 8 Predicted: 5\n",
      "\n",
      "Actual: 6 Predicted: 4\n",
      "\n",
      "Actual: 0 Predicted: 5\n",
      "\n",
      "Actual: 4 Predicted: 9\n",
      "\n",
      "Actual: 2 Predicted: 4\n",
      "\n",
      "Actual: 7 Predicted: 9\n",
      "\n",
      "Actual: 7 Predicted: 8\n",
      "\n",
      "Actual: 7 Predicted: 9\n",
      "\n",
      "Actual: 2 Predicted: 1\n",
      "\n",
      "Actual: 2 Predicted: 8\n",
      "\n",
      "Actual: 8 Predicted: 5\n",
      "\n",
      "Actual: 2 Predicted: 4\n",
      "\n",
      "Actual: 3 Predicted: 9\n",
      "\n",
      "Actual: 6 Predicted: 4\n",
      "\n",
      "Actual: 6 Predicted: 5\n",
      "\n",
      "Actual: 8 Predicted: 6\n",
      "\n",
      "Actual: 4 Predicted: 9\n",
      "\n",
      "Actual: 8 Predicted: 5\n",
      "\n",
      "Actual: 7 Predicted: 3\n",
      "\n",
      "Actual: 8 Predicted: 5\n",
      "\n",
      "Actual: 4 Predicted: 9\n",
      "\n",
      "Actual: 7 Predicted: 2\n",
      "\n",
      "Actual: 7 Predicted: 2\n",
      "\n",
      "Actual: 0 Predicted: 5\n",
      "\n",
      "Actual: 7 Predicted: 2\n",
      "\n",
      "Actual: 7 Predicted: 2\n",
      "\n",
      "Actual: 8 Predicted: 5\n",
      "\n",
      "Actual: 8 Predicted: 5\n",
      "\n",
      "Actual: 7 Predicted: 9\n",
      "\n",
      "Actual: 9 Predicted: 4\n",
      "\n",
      "Actual: 6 Predicted: 5\n",
      "\n",
      "Actual: 0 Predicted: 8\n",
      "\n",
      "Actual: 3 Predicted: 5\n",
      "\n",
      "Actual: 2 Predicted: 7\n",
      "\n",
      "Actual: 6 Predicted: 5\n",
      "\n",
      "Actual: 6 Predicted: 5\n",
      "\n",
      "Actual: 5 Predicted: 0\n",
      "\n",
      "Actual: 5 Predicted: 6\n",
      "\n",
      "Actual: 4 Predicted: 2\n",
      "\n",
      "Actual: 8 Predicted: 5\n",
      "\n",
      "Actual: 2 Predicted: 0\n",
      "\n",
      "Actual: 5 Predicted: 0\n",
      "\n",
      "Actual: 2 Predicted: 0\n",
      "\n",
      "Actual: 9 Predicted: 4\n",
      "\n",
      "Actual: 2 Predicted: 8\n",
      "\n",
      "Actual: 2 Predicted: 7\n",
      "\n",
      "Actual: 6 Predicted: 8\n",
      "\n",
      "Actual: 2 Predicted: 5\n",
      "\n",
      "Actual: 3 Predicted: 9\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Z1, A1, Z2, A2, Z3, A3 = forward_pass(X_test, W1, b1, W2, b2, W3, b3)\n",
    "\n",
    "print(\"Accuracy: {}\".format(np.mean(np.argmax(A3,axis=0) == np.argmax(Y_test,axis=0))*100))\n",
    "\n",
    "for i,x in enumerate(range(A3.shape[1])):\n",
    "    maxpred=np.argmax(A3[:,x])\n",
    "    maxac=np.argmax(Y_test[:,x])\n",
    "\n",
    "    if (maxpred!=maxac):\n",
    "        display_live_number(X_test, i)\n",
    "        # print(\"\\nActual: {}\".format(maxac,end=\"\"),end=\"\")\n",
    "        # print(\" Predicted: {}\".format(maxpred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
