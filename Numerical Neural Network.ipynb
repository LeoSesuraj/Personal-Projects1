{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "# Normalize the input data to the range [0, 1], since there are 255 pixel values\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Flatten the input data (from 28x28 images to 784 feature vectors) and transpose\n",
    "X_train = X_train.reshape(X_train.shape[0], -1).T  # Shape: (784, num_examples)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTIONAL -Display Number\n",
    "\n",
    "def display_live_number(X, index):\n",
    "    # plt.clf()  # Clear the current plot\n",
    "    image = X[:, index].reshape(28, 28)  # Reshape the flattened image back to 28x28\n",
    "    \n",
    "    # Display the image\n",
    "    plt.imshow(image, cmap=\"gray\")\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert labels to one-hot encoding\n",
    "\n",
    "def one_hot_encode(Y, num_classes):\n",
    "    Y = Y.reshape(1, Y.shape[0])  # Reshape to (1, num_examples)\n",
    "    Y_one_hot = np.zeros((num_classes, Y.shape[1]))  # Initialize a zero matrix with shape (num_classes, num_examples)\n",
    "    Y_one_hot[Y.astype(int), np.arange(Y.shape[1])] = 1  # Set the appropriate indices to 1 for each class label\n",
    "    return Y_one_hot\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that initilizes the weights, will only need for training\n",
    "\n",
    "def initialize_parameters(input_size, hidden_size, output_size):\n",
    "    W1 = np.random.randn(hidden_size, input_size) * np.sqrt(2./input_size)  # shape: (hidden_size, input_size)\n",
    "    b1 = np.zeros((hidden_size, 1))  # shape: (hidden_size, 1)\n",
    "    W2 = np.random.randn(hidden_size, hidden_size) * np.sqrt(2./hidden_size)  # shape: (hidden_size, hidden_size)\n",
    "    b2 = np.zeros((hidden_size, 1))  # shape: (hidden_size, 1)\n",
    "    W3 = np.random.randn(output_size, hidden_size) * np.sqrt(2./hidden_size)  # shape: (output_size, hidden_size)\n",
    "    b3 = np.zeros((output_size, 1))  # shape: (output_size, 1)\n",
    "\n",
    "    return W1, b1, W2, b2, W3, b3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relu call to change data from linear representation to a more complex version so the deep learning process is more intricate\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0, Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for the last layer and for the output, the softmax will convert Z into probabilities based on the formula\n",
    "\n",
    "def softmax(Z):\n",
    "    # to highlight the more confident probabilities and to make positive probabilities:\n",
    "    exp_Z = np.exp(Z - np.max(Z, axis=0, keepdims=True))  # For numerical stability\n",
    "    return exp_Z / np.sum(exp_Z, axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward pass algorithm\n",
    "\n",
    "def forward_pass(X, W1, b1, W2, b2, W3, b3):\n",
    "    # First hidden layer (Input -> Hidden Layer 1)\n",
    "    Z1 = np.dot(W1, X) + b1  # Z1 shape: (hidden_size, num_examples)\n",
    "    A1 = relu(Z1)\n",
    "\n",
    "    # Second hidden layer (Hidden Layer 1 -> Hidden Layer 2)\n",
    "    Z2 = np.dot(W2, A1) + b2  # Z2 shape: (hidden_size, num_examples)\n",
    "    A2 = relu(Z2)\n",
    "    \n",
    "    # Output layer (Hidden Layer 2 -> Output)\n",
    "    Z3 = np.dot(W3, A2) + b3  # Z3 shape: (output_size, num_examples)\n",
    "    A3 = softmax(Z3)\n",
    "\n",
    "    return Z1, A1, Z2, A2, Z3, A3  # Return the output probabilities and intermediate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss calculation that finds the loss and how well the predicted values align with the true values\n",
    "\n",
    "def compute_loss(A3, Y):\n",
    "    m = Y.shape[1]  # Number of examples\n",
    "    epsilon = 1e-15  # Small constant to avoid log(0)\n",
    "    \n",
    "    # Compute the log likelihood for the correct class (one-hot encoded labels)\n",
    "    log_probs = np.multiply(np.log(A3 + epsilon), Y)  # Element-wise multiplication\n",
    "    cost = -np.sum(log_probs) / m  # Average loss over the batch\n",
    "    \n",
    "    print(\"Accuracy: {}\".format(np.mean(np.argmax(A3,axis=0) == np.argmax(Y,axis=0))*100))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#derivative function that returns 0 if the Z is less than 0\n",
    "def relu_derivative(Z):\n",
    "    return Z > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backpropagation, going backwards in the neural network and calculating the slope and direction to move it to adjust based on the loss\n",
    "#Backpropagation only finds the direction (slope) of the movement, not by actually finding how much it needs to move, that's the learning rate.\n",
    "\n",
    "def backpropagation(X, Y, parameters, cache):\n",
    "    m = X.shape[1]  # Number of examples (batch size)\n",
    "\n",
    "    # cache is a type of memory retreival, where we are accessing the values of the following:\n",
    "    A1 = cache['A1']  # relu version of weighted matrix layer 1\n",
    "    A2 = cache['A2']  # relu version of weighted matrix layer 2\n",
    "    A3 = cache['A3']  # softmax version of weighted matrix layer 3 (output layer)\n",
    "    Z1 = cache['Z1']  # weighted matrix pre-relu layer 1\n",
    "    Z2 = cache['Z2']  # weighted matrix pre-relu layer 2\n",
    "\n",
    "    W1, W2, W3 = parameters['W1'], parameters['W2'], parameters['W3']\n",
    "\n",
    "    # Layer 3: Compute the gradient for layer 3, output layer\n",
    "    dZ3 = A3 - Y\n",
    "\n",
    "    # Layer 3: Compute the gradients for W3 and b3 (output layer)\n",
    "    dW3 = np.dot(dZ3, A2.T) / m  # Gradient of W3\n",
    "    db3 = np.sum(dZ3, axis=1, keepdims=True) / m  # Gradient of b3\n",
    "\n",
    "    # Layer 2: Backpropagate the error to the second hidden layer (Z2)\n",
    "    dZ2 = np.dot(W3.T, dZ3) * relu_derivative(Z2)  # Gradient of Z2\n",
    "\n",
    "    # Layer 2: Compute the gradients for W2 and b2 (second hidden layer)\n",
    "    dW2 = np.dot(dZ2, A1.T) / m  # Gradient of W2\n",
    "    db2 = np.sum(dZ2, axis=1, keepdims=True) / m  # Gradient of b2\n",
    "\n",
    "    # Layer 1: Backpropagate the error to the first hidden layer (Z1)\n",
    "    dZ1 = np.dot(W2.T, dZ2) * relu_derivative(Z1)  # Gradient of Z1\n",
    "    \n",
    "    # Layer 1: Compute the gradients for W1 and b1 (first hidden layer)\n",
    "    dW1 = np.dot(dZ1, X.T) / m  # Gradient of W1\n",
    "    db1 = np.sum(dZ1, axis=1, keepdims=True) / m  # Gradient of b1\n",
    "\n",
    "    gradients = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2, \"dW3\": dW3, \"db3\": db3}\n",
    "\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the actual gradient descent step where the weights and biases are updated based on a learning rate (usually beginning at 0.01)\n",
    "#The gradient descent step will repeat multiple times, each time adjusting the weight and bias closer and closer for precision\n",
    "\n",
    "def gradient_descent(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, learning_rate):\n",
    "    # Update weights and biases using gradient descent\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    W3 -= learning_rate * dW3\n",
    "    b3 -= learning_rate * db3\n",
    "    \n",
    "    return W1, b1, W2, b2, W3, b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the algorithm\n",
    "def train(X, Y, input_size, hidden_layer_size, output_size, num_iterations, learning_rate, batch_size=128):\n",
    "    # Initialize the parameters (weights and biases)\n",
    "    W1, b1, W2, b2, W3, b3 = initialize_parameters(input_size, hidden_layer_size, output_size)\n",
    "    m = X.shape[1]\n",
    "    costs = []\n",
    "    \n",
    "    # Train for the specified number of iterations\n",
    "    for iteration in range(num_iterations):\n",
    "        # Mini-batch implementation\n",
    "        for batch_start in range(0, m, batch_size):\n",
    "            batch_end = min(batch_start + batch_size, m)\n",
    "            X_batch = X[:, batch_start:batch_end]\n",
    "            Y_batch = Y[:, batch_start:batch_end]\n",
    "            \n",
    "            # Forward propagation\n",
    "            Z1, A1, Z2, A2, Z3, A3 = forward_pass(X_batch, W1, b1, W2, b2, W3, b3)\n",
    "            \n",
    "            # Compute loss for monitoring\n",
    "            if batch_start == 0 and (iteration+1) % 1 == 0:\n",
    "                cost = compute_loss(A3, Y_batch)\n",
    "                costs.append(cost)\n",
    "                print(f\"Iteration {iteration+1}, Cost: {cost}\")\n",
    "            \n",
    "            # Backpropagation to compute gradients\n",
    "            gradients = backpropagation(X_batch, Y_batch, \n",
    "                                      {\"W1\": W1, \"W2\": W2, \"W3\": W3},\n",
    "                                      {'A1': A1, 'A2': A2, 'A3': A3, 'Z1': Z1, 'Z2': Z2})\n",
    "            \n",
    "            # Update parameters using gradient descent\n",
    "            W1, b1, W2, b2, W3, b3 = gradient_descent(\n",
    "                W1, b1, W2, b2, W3, b3,\n",
    "                gradients['dW1'], gradients['db1'],\n",
    "                gradients['dW2'], gradients['db2'],\n",
    "                gradients['dW3'], gradients['db3'],\n",
    "                learning_rate\n",
    "            )\n",
    "    \n",
    "    return W1, b1, W2, b2, W3, b3, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 4.6875\n",
      "Iteration 1, Cost: 2.595135238137568\n",
      "Accuracy: 96.09375\n",
      "Iteration 2, Cost: 0.18835674911940375\n",
      "Accuracy: 96.875\n",
      "Iteration 3, Cost: 0.13526032771291638\n",
      "Accuracy: 97.65625\n",
      "Iteration 4, Cost: 0.11254291247774262\n",
      "Accuracy: 97.65625\n",
      "Iteration 5, Cost: 0.09815459399302283\n",
      "Accuracy: 99.21875\n",
      "Iteration 6, Cost: 0.08550724614668043\n",
      "Accuracy: 99.21875\n",
      "Iteration 7, Cost: 0.07512210948069893\n",
      "Accuracy: 99.21875\n",
      "Iteration 8, Cost: 0.06777156859824787\n",
      "Accuracy: 99.21875\n",
      "Iteration 9, Cost: 0.061210619064195435\n",
      "Accuracy: 99.21875\n",
      "Iteration 10, Cost: 0.05641322928600133\n"
     ]
    }
   ],
   "source": [
    "#Main Class to train and run\n",
    "\n",
    "input_size = 784  # each image is 28x28 pixels\n",
    "hidden_layer_size = 128  # number of neurons in each hidden layer (increased from 20 for better performance)\n",
    "output_size = 10  # 0-9 possible values\n",
    "num_iterations = 10  # running the training 10 times to modify the matrices accordingly\n",
    "learning_rate = 0.1\n",
    "batch_size = 128\n",
    "\n",
    "# Prepare data with one-hot encoding\n",
    "Y_train = one_hot_encode(Y_train, output_size)\n",
    "Y_test = one_hot_encode(Y_test, output_size)\n",
    "\n",
    "# Train the model\n",
    "W1, b1, W2, b2, W3, b3, costs = train(X_train, Y_train, input_size, hidden_layer_size, output_size, num_iterations, learning_rate, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.2\n"
     ]
    }
   ],
   "source": [
    "Z1, A1, Z2, A2, Z3, A3 = forward_pass(X_test, W1, b1, W2, b2, W3, b3)\n",
    "\n",
    "print(\"Accuracy: {}\".format(np.mean(np.argmax(A3,axis=0) == np.argmax(Y_test,axis=0))*100))\n",
    "\n",
    "# for i,x in enumerate(range(A3.shape[1])):\n",
    "#     maxpred=np.argmax(A3[:,x])\n",
    "#     maxac=np.argmax(Y_test[:,x])\n",
    "\n",
    "#     if (maxpred!=maxac):\n",
    "#         display_live_number(X_test, i)\n",
    "#         print(\"\\nActual: {}\".format(maxac,end=\"\"),end=\"\")\n",
    "#         print(\" Predicted: {}\".format(maxpred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
